{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "import os\n",
    "from langchain_openai import  ChatOpenAI\n",
    "model=ChatOpenAI(model='deepseek-chat',api_key=os.getenv('Deepseel_API_KEY'),base_url='https://api.deepseek.com/v1')\n",
    "from langchain.memory import ConversationKGMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': '', 'entities': {'小明，小黄，小绿': ''}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=ConversationEntityMemory(llm=model)\n",
    "_input={\n",
    "    \"input\": \"小明，小黄，小绿谁不一样\",\n",
    "}\n",
    "m.load_memory_variables(_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save_context(\n",
    "_input,\n",
    "{\n",
    "    \"output\":\"小明\"\n",
    "}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: 小明，小黄，小绿谁不一样\\nAI: 小明',\n",
       " 'entities': {'小明': '', '小黄': '', '小绿': ''}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.load_memory_variables({\"input\":\"谁不一样\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "长对话保存的处理方式：总结摘要，以及计算token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yks\\AppData\\Local\\Temp\\ipykernel_11528\\2197052748.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  m = ConversationSummaryMemory(llm=model)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': 'The human asked the AI to identify differences among 小明, 小黄, and 小绿. The AI determined that 小明 is与众不同 because he is not a color.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "'''总结上下文形成摘要'''\n",
    "\n",
    "m = ConversationSummaryMemory(llm=model)\n",
    "m.save_context(\n",
    "    {\n",
    "        \"input\": \"小明，小黄，小绿谁不一样\",\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"小明\"\n",
    "    }\n",
    ")\n",
    "m.save_context(\n",
    "    {\n",
    "        \"input\": \"为什么小明不一样\",\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"因为小明不是颜色\"\n",
    "    }\n",
    ")\n",
    "\n",
    "m.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The human asked the AI to identify which one among Xiao Ming, Xiao Huang, and Xiao Lu is different. The AI pointed out that Xiao Ming is the与众不同 (different) one. When asked why, the AI explained that Xiao Ming is not a color, unlike Xiao Huang and Xiao Lu.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesage=m.chat_memory.messages\n",
    "m.predict_new_summary(mesage,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history=ChatMessageHistory()\n",
    "history.add_user_message('小明，小黄，小绿谁不一样')\n",
    "history.add_ai_message('小明')\n",
    "\n",
    "memory=ConversationSummaryMemory(\n",
    "    llm=model,\n",
    "    return_messages=True,\n",
    "    chat_memory=history\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yks\\AppData\\Local\\Temp\\ipykernel_27180\\1172335075.py:7: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory=ConversationTokenBufferMemory(\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model cl100k_base. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m      7\u001b[0m memory\u001b[38;5;241m=\u001b[39mConversationTokenBufferMemory(\n\u001b[0;32m      8\u001b[0m     llm\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      9\u001b[0m     return_messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     10\u001b[0m     max_token_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_context(\n\u001b[0;32m     13\u001b[0m     {\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m小明，小黄，小绿谁不一样\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     },\n\u001b[0;32m     16\u001b[0m     {\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m小明\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m     }\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_context(\n\u001b[0;32m     21\u001b[0m     {\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m为什么小明不一样\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     }\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     28\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_context(\n\u001b[0;32m     29\u001b[0m     {\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m还因为什么？\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     }\n\u001b[0;32m     35\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\langchain\\memory\\token_buffer.py:67\u001b[0m, in \u001b[0;36mConversationTokenBufferMemory.save_context\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Prune buffer if it exceeds max token limit\u001b[39;00m\n\u001b[0;32m     66\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39mmessages\n\u001b[1;32m---> 67\u001b[0m curr_buffer_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mget_num_tokens_from_messages(buffer)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr_buffer_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_token_limit:\n\u001b[0;32m     69\u001b[0m     pruned_memory \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:960\u001b[0m, in \u001b[0;36mBaseChatOpenAI.get_num_tokens_from_messages\u001b[1;34m(self, messages, tools)\u001b[0m\n\u001b[0;32m    958\u001b[0m     tokens_per_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    961\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    962\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://platform.openai.com/docs/guides/text-generation/managing-tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for information on how messages are converted to tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    965\u001b[0m     )\n\u001b[0;32m    966\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    967\u001b[0m messages_dict \u001b[38;5;241m=\u001b[39m [_convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: get_num_tokens_from_messages() is not presently implemented for model cl100k_base. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "#对话持续进行且内容很多的适合 可以使用ConversationSummaryBufferMemory\n",
    "#可以通过token的阈值进行自动摘要，比较久的优先进行摘要 目前通过报错来看阿里云的模型不支持计算 token数量\n",
    "#目前这个类还无法使用\n",
    "\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from openai import OpenAI\n",
    "memory=ConversationTokenBufferMemory(\n",
    "    llm=model,\n",
    "    return_messages=True,\n",
    "    max_token_limit=10\n",
    ")\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"小明，小黄，小绿谁不一样\",\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"小明\"\n",
    "    }\n",
    ")\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"为什么小明不一样\",\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"因为小明不是颜色\"\n",
    "    }\n",
    ")\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"还因为什么？\",\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"因为小明笔画最少\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yks\\AppData\\Local\\Temp\\ipykernel_26792\\4137319174.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model cl100k_base. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 模拟添加多轮对话（自动触发摘要）\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m---> 11\u001b[0m     memory\u001b[38;5;241m.\u001b[39msave_context({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m用户的问题\u001b[39m\u001b[38;5;124m\"\u001b[39m}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m模型的回答\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 查看当前内存内容\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(memory\u001b[38;5;241m.\u001b[39mload_memory_variables({}))\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\langchain\\memory\\summary_buffer.py:96\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.save_context\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave_context(inputs, outputs)\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprune()\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\langchain\\memory\\summary_buffer.py:108\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.prune\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prune buffer if it exceeds max token limit\"\"\"\u001b[39;00m\n\u001b[0;32m    107\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39mmessages\n\u001b[1;32m--> 108\u001b[0m curr_buffer_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mget_num_tokens_from_messages(buffer)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr_buffer_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_token_limit:\n\u001b[0;32m    110\u001b[0m     pruned_memory \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:960\u001b[0m, in \u001b[0;36mBaseChatOpenAI.get_num_tokens_from_messages\u001b[1;34m(self, messages, tools)\u001b[0m\n\u001b[0;32m    958\u001b[0m     tokens_per_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    961\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    962\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://platform.openai.com/docs/guides/text-generation/managing-tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for information on how messages are converted to tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    965\u001b[0m     )\n\u001b[0;32m    966\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    967\u001b[0m messages_dict \u001b[38;5;241m=\u001b[39m [_convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: get_num_tokens_from_messages() is not presently implemented for model cl100k_base. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=model,\n",
    "    max_token_limit=400  # 触发摘要的 Token 阈值\n",
    ")\n",
    "\n",
    "# 模拟添加多轮对话（自动触发摘要）\n",
    "for _ in range(10):\n",
    "    memory.save_context({\"input\": \"用户的问题\"}, {\"output\": \"模型的回答\"})\n",
    "\n",
    "# 查看当前内存内容\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: 截断历史\\nAI: AI: 模型回答 44\\nHuman: 用户问题 45\\nAI: 模型回答 45\\nHuman: 用户问题 46\\nAI: 模型回答 46\\nHuman: 用户问题 47\\nAI: 模型回答 47\\nHuman: 用户问题 48\\nAI: 模型回答 48\\nHuman: 用户问题 49\\nAI: 模型回答 49'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "import tiktoken\n",
    "\n",
    "# 初始化 LLM 和 Memory\n",
    "llm =model\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# 初始化 Token 计算工具\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")  # 使用 GPT-3.5/GPT-4 的编码\n",
    "\n",
    "# 定义 Token 阈值\n",
    "MAX_TOKEN_LIMIT = 100\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"统计文本的 Token 数量\"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def prune_memory_if_needed(memory):\n",
    "    \"\"\"检查并截断内存内容\"\"\"\n",
    "    buffer = memory.load_memory_variables({})[\"history\"]\n",
    "    token_count = count_tokens(buffer)\n",
    "    \n",
    "    if token_count > MAX_TOKEN_LIMIT:\n",
    "        # 截断逻辑：保留最近的对话\n",
    "        messages = buffer.split(\"\\n\")\n",
    "        while token_count > MAX_TOKEN_LIMIT and len(messages) > 1:\n",
    "            messages.pop(0)  # 移除最早的消息\n",
    "            buffer = \"\\n\".join(messages)\n",
    "            token_count = count_tokens(buffer)\n",
    "        \n",
    "        # 更新内存\n",
    "        memory.clear()\n",
    "        memory.save_context({\"input\": \"截断历史\"}, {\"output\": buffer})\n",
    "\n",
    "# 模拟添加多轮对话\n",
    "for i in range(50):\n",
    "    memory.save_context({\"input\": f\"用户问题 {i}\"}, {\"output\": f\"模型回答 {i}\"})\n",
    "    prune_memory_if_needed(memory)  # 检查并截断\n",
    "\n",
    "# 查看当前内存内容\n",
    "print(memory.load_memory_variables({}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
