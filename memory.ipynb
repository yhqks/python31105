{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "import os\n",
    "from langchain_openai import  ChatOpenAI\n",
    "model=ChatOpenAI(model='qwen-72b-chat',api_key=os.getenv('DASHSCOPE_API_KEY'),base_url='https://dashscope.aliyuncs.com/compatible-mode/v1')\n",
    "from langchain.memory import ConversationKGMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': '', 'entities': {'小明，小黄，小绿': ''}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=ConversationEntityMemory(llm=model)\n",
    "_input={\n",
    "    \"input\": \"小明，小黄，小绿谁不一样\",\n",
    "}\n",
    "m.load_memory_variables(_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save_context(\n",
    "_input,\n",
    "{\n",
    "    \"output\":\"小明\"\n",
    "}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: 小明，小黄，小绿谁不一样\\nAI: 小明',\n",
       " 'entities': {'小明': '', '小黄': '', '小绿': ''}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.load_memory_variables({\"input\":\"谁不一样\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "长对话保存的处理方式：总结摘要，以及计算token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yks\\AppData\\Local\\Temp\\ipykernel_11528\\2197052748.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  m = ConversationSummaryMemory(llm=model)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': 'The human asked the AI to identify differences among 小明, 小黄, and 小绿. The AI determined that 小明 is与众不同 because he is not a color.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "'''总结上下文形成摘要'''\n",
    "\n",
    "m = ConversationSummaryMemory(llm=model)\n",
    "m.save_context(\n",
    "    {\n",
    "        \"input\": \"小明，小黄，小绿谁不一样\",\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"小明\"\n",
    "    }\n",
    ")\n",
    "m.save_context(\n",
    "    {\n",
    "        \"input\": \"为什么小明不一样\",\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"因为小明不是颜色\"\n",
    "    }\n",
    ")\n",
    "\n",
    "m.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The human asked the AI to identify which one among Xiao Ming, Xiao Huang, and Xiao Lu is different. The AI pointed out that Xiao Ming is the与众不同 (different) one. When asked why, the AI explained that Xiao Ming is not a color, unlike Xiao Huang and Xiao Lu.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesage=m.chat_memory.messages\n",
    "m.predict_new_summary(mesage,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history=ChatMessageHistory()\n",
    "history.add_user_message('小明，小黄，小绿谁不一样')\n",
    "history.add_ai_message('小明')\n",
    "\n",
    "memory=ConversationSummaryMemory(\n",
    "    llm=model,\n",
    "    return_messages=True,\n",
    "    chat_memory=history\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yks\\AppData\\Local\\Temp\\ipykernel_27180\\1172335075.py:7: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory=ConversationTokenBufferMemory(\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model cl100k_base. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m      7\u001b[0m memory\u001b[38;5;241m=\u001b[39mConversationTokenBufferMemory(\n\u001b[0;32m      8\u001b[0m     llm\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      9\u001b[0m     return_messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     10\u001b[0m     max_token_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_context(\n\u001b[0;32m     13\u001b[0m     {\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m小明，小黄，小绿谁不一样\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     },\n\u001b[0;32m     16\u001b[0m     {\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m小明\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m     }\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_context(\n\u001b[0;32m     21\u001b[0m     {\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m为什么小明不一样\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     }\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     28\u001b[0m memory\u001b[38;5;241m.\u001b[39msave_context(\n\u001b[0;32m     29\u001b[0m     {\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m还因为什么？\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     }\n\u001b[0;32m     35\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\langchain\\memory\\token_buffer.py:67\u001b[0m, in \u001b[0;36mConversationTokenBufferMemory.save_context\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Prune buffer if it exceeds max token limit\u001b[39;00m\n\u001b[0;32m     66\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39mmessages\n\u001b[1;32m---> 67\u001b[0m curr_buffer_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mget_num_tokens_from_messages(buffer)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr_buffer_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_token_limit:\n\u001b[0;32m     69\u001b[0m     pruned_memory \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:960\u001b[0m, in \u001b[0;36mBaseChatOpenAI.get_num_tokens_from_messages\u001b[1;34m(self, messages, tools)\u001b[0m\n\u001b[0;32m    958\u001b[0m     tokens_per_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    961\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    962\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://platform.openai.com/docs/guides/text-generation/managing-tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for information on how messages are converted to tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    965\u001b[0m     )\n\u001b[0;32m    966\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    967\u001b[0m messages_dict \u001b[38;5;241m=\u001b[39m [_convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: get_num_tokens_from_messages() is not presently implemented for model cl100k_base. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "#对话持续进行且内容很多的适合 可以使用ConversationSummaryBufferMemory\n",
    "#可以通过token的阈值进行自动摘要，比较久的优先进行摘要 目前通过报错来看阿里云的模型不支持计算 token数量\n",
    "#目前这个类还无法使用\n",
    "\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from openai import OpenAI\n",
    "memory=ConversationTokenBufferMemory(\n",
    "    llm=model,\n",
    "    return_messages=True,\n",
    "    max_token_limit=10\n",
    ")\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"小明，小黄，小绿谁不一样\",\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"小明\"\n",
    "    }\n",
    ")\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"为什么小明不一样\",\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"因为小明不是颜色\"\n",
    "    }\n",
    ")\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"还因为什么？\",\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"因为小明笔画最少\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
