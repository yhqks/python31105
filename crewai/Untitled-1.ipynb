{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:LiteLLM call failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=DeepSeek-R1-Distill-Qwen-1.5B\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92m数据分析师\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m分析数据并提供洞察\u001b[00m\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\u001b[91m Error during LLM call: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=DeepSeek-R1-Distill-Qwen-1.5B\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[00m\n",
      "\u001b[91m An unknown error occurred. Please check the details below.\u001b[00m\n",
      "\u001b[91m Error details: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=DeepSeek-R1-Distill-Qwen-1.5B\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[00m\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=DeepSeek-R1-Distill-Qwen-1.5B\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 38\u001b[0m\n\u001b[0;32m     27\u001b[0m task\u001b[38;5;241m=\u001b[39mTask(\n\u001b[0;32m     28\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m分析数据并提供洞察\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 任务描述\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent,  \u001b[38;5;66;03m# 任务执行者\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     expected_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m一份详细的数据分析报告，包含趋势、模式和洞察。\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m test\u001b[38;5;241m=\u001b[39mCrew(\n\u001b[0;32m     33\u001b[0m     agents\u001b[38;5;241m=\u001b[39m[agent],  \u001b[38;5;66;03m# 机器人列表\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     tasks\u001b[38;5;241m=\u001b[39m[task],  \u001b[38;5;66;03m# 任务列表\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     process\u001b[38;5;241m=\u001b[39mProcess\u001b[38;5;241m.\u001b[39msequential,  \u001b[38;5;66;03m# 任务处理方式\u001b[39;00m\n\u001b[0;32m     36\u001b[0m )\n\u001b[1;32m---> 38\u001b[0m res\u001b[38;5;241m=\u001b[39mtest\u001b[38;5;241m.\u001b[39mkickoff()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\crewai\\crew.py:640\u001b[0m, in \u001b[0;36mCrew.kickoff\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m metrics: List[UsageMetrics] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39msequential:\n\u001b[1;32m--> 640\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sequential_process()\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39mhierarchical:\n\u001b[0;32m    642\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_hierarchical_process()\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\crewai\\crew.py:752\u001b[0m, in \u001b[0;36mCrew._run_sequential_process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CrewOutput:\n\u001b[0;32m    751\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 752\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_tasks(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\crewai\\crew.py:850\u001b[0m, in \u001b[0;36mCrew._execute_tasks\u001b[1;34m(self, tasks, start_index, was_replayed)\u001b[0m\n\u001b[0;32m    847\u001b[0m     futures\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m    849\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_context(task, task_outputs)\n\u001b[1;32m--> 850\u001b[0m task_output \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mexecute_sync(\n\u001b[0;32m    851\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent_to_use,\n\u001b[0;32m    852\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m    853\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools_for_task,\n\u001b[0;32m    854\u001b[0m )\n\u001b[0;32m    855\u001b[0m task_outputs\u001b[38;5;241m.\u001b[39mappend(task_output)\n\u001b[0;32m    856\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_task_result(task, task_output)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\crewai\\task.py:310\u001b[0m, in \u001b[0;36mTask.execute_sync\u001b[1;34m(self, agent, context, tools)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_sync\u001b[39m(\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    305\u001b[0m     agent: Optional[BaseAgent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    306\u001b[0m     context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    307\u001b[0m     tools: Optional[List[BaseTool]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskOutput:\n\u001b[0;32m    309\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_core(agent, context, tools)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\crewai\\task.py:454\u001b[0m, in \u001b[0;36mTask._execute_core\u001b[1;34m(self, agent, context, tools)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    453\u001b[0m crewai_event_bus\u001b[38;5;241m.\u001b[39memit(\u001b[38;5;28mself\u001b[39m, TaskFailedEvent(error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\crewai\\task.py:374\u001b[0m, in \u001b[0;36mTask._execute_core\u001b[1;34m(self, agent, context, tools)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_by_agents\u001b[38;5;241m.\u001b[39madd(agent\u001b[38;5;241m.\u001b[39mrole)\n\u001b[0;32m    373\u001b[0m crewai_event_bus\u001b[38;5;241m.\u001b[39memit(\u001b[38;5;28mself\u001b[39m, TaskStartedEvent(context\u001b[38;5;241m=\u001b[39mcontext))\n\u001b[1;32m--> 374\u001b[0m result \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mexecute_task(\n\u001b[0;32m    375\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    376\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m    377\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    380\u001b[0m pydantic_output, json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_output(result)\n\u001b[0;32m    381\u001b[0m task_output \u001b[38;5;241m=\u001b[39m TaskOutput(\n\u001b[0;32m    382\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    383\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    389\u001b[0m     output_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_format(),\n\u001b[0;32m    390\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\crewai\\agent.py:266\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[1;34m(self, task, context, tools)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[0;32m    258\u001b[0m     crewai_event_bus\u001b[38;5;241m.\u001b[39memit(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         event\u001b[38;5;241m=\u001b[39mAgentExecutionErrorEvent(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    264\u001b[0m         ),\n\u001b[0;32m    265\u001b[0m     )\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retry_limit:\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\crewai\\agent.py:247\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[1;34m(self, task, context, tools)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     crewai_event_bus\u001b[38;5;241m.\u001b[39memit(\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    240\u001b[0m         event\u001b[38;5;241m=\u001b[39mAgentExecutionStartedEvent(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m         ),\n\u001b[0;32m    246\u001b[0m     )\n\u001b[1;32m--> 247\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_executor\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    248\u001b[0m         {\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: task_prompt,\n\u001b[0;32m    250\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_names\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_executor\u001b[38;5;241m.\u001b[39mtools_names,\n\u001b[0;32m    251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_executor\u001b[38;5;241m.\u001b[39mtools_description,\n\u001b[0;32m    252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_for_human_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: task\u001b[38;5;241m.\u001b[39mhuman_input,\n\u001b[0;32m    253\u001b[0m         }\n\u001b[0;32m    254\u001b[0m     )[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:119\u001b[0m, in \u001b[0;36mCrewAgentExecutor.invoke\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_unknown_error(e)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:108\u001b[0m, in \u001b[0;36mCrewAgentExecutor.invoke\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_for_human_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_for_human_input\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 108\u001b[0m     formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke_loop()\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    111\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent failed to reach a final answer. This is likely a bug - please report it.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    112\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    113\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:166\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_context_length_exceeded(e):\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_context_length()\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:146\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enforce_rpm_limit()\n\u001b[1;32m--> 146\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_llm_response()\n\u001b[0;32m    147\u001b[0m formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_llm_response(answer)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_answer, AgentAction):\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:216\u001b[0m, in \u001b[0;36mCrewAgentExecutor._get_llm_response\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    213\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during LLM call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    214\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    215\u001b[0m     )\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m answer:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    220\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived None or empty response from LLM call.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:207\u001b[0m, in \u001b[0;36mCrewAgentExecutor._get_llm_response\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call the LLM and return the response, handling any invalid responses.\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mcall(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages,\n\u001b[0;32m    209\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m    210\u001b[0m     )\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    213\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during LLM call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    214\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    215\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\crewai\\llm.py:739\u001b[0m, in \u001b[0;36mLLM.call\u001b[1;34m(self, messages, tools, callbacks, available_functions)\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_streaming_response(\n\u001b[0;32m    736\u001b[0m             params, callbacks, available_functions\n\u001b[0;32m    737\u001b[0m         )\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 739\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_non_streaming_response(\n\u001b[0;32m    740\u001b[0m             params, callbacks, available_functions\n\u001b[0;32m    741\u001b[0m         )\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    744\u001b[0m     crewai_event_bus\u001b[38;5;241m.\u001b[39memit(\n\u001b[0;32m    745\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    746\u001b[0m         event\u001b[38;5;241m=\u001b[39mLLMCallFailedEvent(error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)),\n\u001b[0;32m    747\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\crewai\\llm.py:575\u001b[0m, in \u001b[0;36mLLM._handle_non_streaming_response\u001b[1;34m(self, params, callbacks, available_functions)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Handle a non-streaming response from the LLM.\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \n\u001b[0;32m    566\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;124;03m    str: The response text\u001b[39;00m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# --- 1) Make the completion call\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m response \u001b[38;5;241m=\u001b[39m litellm\u001b[38;5;241m.\u001b[39mcompletion(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;66;03m# --- 2) Extract response message and content\u001b[39;00m\n\u001b[0;32m    578\u001b[0m response_message \u001b[38;5;241m=\u001b[39m cast(Choices, cast(ModelResponse, response)\u001b[38;5;241m.\u001b[39mchoices)[\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    580\u001b[0m ]\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\litellm\\utils.py:1154\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[0;32m   1151\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[0;32m   1152\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[0;32m   1153\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[1;32m-> 1154\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\litellm\\utils.py:1032\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1030\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[1;32m-> 1032\u001b[0m result \u001b[38;5;241m=\u001b[39m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1033\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\litellm\\main.py:3068\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   3065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m   3066\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   3067\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[1;32m-> 3068\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[0;32m   3069\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   3070\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[0;32m   3071\u001b[0m         original_exception\u001b[38;5;241m=\u001b[39me,\n\u001b[0;32m   3072\u001b[0m         completion_kwargs\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   3073\u001b[0m         extra_kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   3074\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\litellm\\main.py:979\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m    977\u001b[0m     model \u001b[38;5;241m=\u001b[39m deployment_id\n\u001b[0;32m    978\u001b[0m     custom_llm_provider \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 979\u001b[0m model, custom_llm_provider, dynamic_api_key, api_base \u001b[38;5;241m=\u001b[39m get_llm_provider(\n\u001b[0;32m    980\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    981\u001b[0m     custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[0;32m    982\u001b[0m     api_base\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[0;32m    983\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[0;32m    984\u001b[0m )\n\u001b[0;32m    986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    987\u001b[0m     provider_specific_header \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m provider_specific_header[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_llm_provider\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m custom_llm_provider\n\u001b[0;32m    989\u001b[0m ):\n\u001b[0;32m    990\u001b[0m     headers\u001b[38;5;241m.\u001b[39mupdate(provider_specific_header[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\litellm\\litellm_core_utils\\get_llm_provider_logic.py:356\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[1;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError):\n\u001b[1;32m--> 356\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m         error_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    359\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         )\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\litellm\\litellm_core_utils\\get_llm_provider_logic.py:333\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[1;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[0;32m    331\u001b[0m     error_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Pass model as E.g. For \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHuggingface\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m inference endpoints pass in `completion(model=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuggingface/starcoder\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;66;03m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    334\u001b[0m         message\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[0;32m    335\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    336\u001b[0m         response\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mResponse(\n\u001b[0;32m    337\u001b[0m             status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m,\n\u001b[0;32m    338\u001b[0m             content\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[0;32m    339\u001b[0m             request\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mRequest(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/BerriAI/litellm\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    340\u001b[0m         ),\n\u001b[0;32m    341\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    342\u001b[0m     )\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(api_base, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi base needs to be a string. api_base=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(api_base)\n\u001b[0;32m    346\u001b[0m     )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=DeepSeek-R1-Distill-Qwen-1.5B\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
     ]
    }
   ],
   "source": [
    "from json import load\n",
    "from tabnanny import verbose\n",
    "import time\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.chat_models.base import BaseChatOpenAI\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "from dotenv import load_dotenv\n",
    "from crewai import Agent,Task,Crew,Process\n",
    "import os\n",
    "# OPENAI_API_KEY = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "# serpapi=os.getenv('serpapi')\n",
    "# print(serpapi)\n",
    "# load_dotenv()\n",
    "API_KEY=os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "llm = ChatOpenAI(model='DeepSeek-R1-Distill-Qwen-1.5B', api_key=API_KEY, base_url='https://dashscope.aliyuncs.com/compatible-mode/v1')\n",
    "agent = Agent(\n",
    "    role=\"数据分析师\",  # 角色\n",
    "    goal=\"分析数据并提供洞察\",  # 目标\n",
    "    backstory=\"你是一名经验丰富的数据分析师，擅长从数据中发现趋势和模式。\",  # 背景故事\n",
    "    tools=[],  # 可选工具列表\n",
    "    verbose=True,  # 是否输出详细信息\n",
    "    llm=llm,  # LLM模型\n",
    ")\n",
    "task=Task(\n",
    "    description=\"分析数据并提供洞察\",  # 任务描述\n",
    "    agent=agent,  # 任务执行者\n",
    "    expected_output=\"一份详细的数据分析报告，包含趋势、模式和洞察。\",\n",
    ")\n",
    "test=Crew(\n",
    "    agents=[agent],  # 机器人列表\n",
    "    tasks=[task],  # 任务列表\n",
    "    process=Process.sequential,  # 任务处理方式\n",
    "    knowledge=\n",
    ")\n",
    "\n",
    "res=test.kickoff()\n",
    "print(res)\n",
    "# tools = load_tools(['serpapi','llm-math'],llm=llm,serpapi_api_key=serpapi)\n",
    "\n",
    "# agent=initialize_agent(\n",
    "#     tools,\n",
    "#     llm,\n",
    "#     agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# print(agent.run(\"现在中国有多少省份，面积最大和最小的省份之间的面积差距有多大\"))\n",
    "\n",
    "# tools = load_tools(['dalle-image-generator'])\n",
    "\n",
    "# agent=initialize_agent(\n",
    "#     tools,\n",
    "#     llm,\n",
    "#     agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# print(agent.run('一个长的很帅的人'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
